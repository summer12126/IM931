{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "zwk9nkV2nxeV"
      },
      "id": "zwk9nkV2nxeV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTUcrmg79jcF"
      },
      "source": [
        "# In today's lab, we'll cover two topics that will be useful in your assignment: Overfitting and Convolutional layers\n",
        "\n",
        "## Overfitting\n",
        "\n",
        "We'll see a few ways of reducing overfitting, including:\n",
        "* Reducing the capacity of the network.\n",
        "* Adding weight regularization.\n",
        "* Incorporating dropout.\n",
        "\n",
        "In many of the examples we've seen so far, the performance of our model on the validation data peaked after a few epochs and would then start degrading, even though the performance on the training data continued to improve. This is known as _overfitting_ to the training data. Overfitting happens in every single machine learning problem, and learning how to deal with it is essential to mastering machine learning.\n",
        "\n",
        "To start with, we'll continue to work with the imdb movie review dataset to investigate overfitting. Look back at last week's lab if you can't remember what any of the following code is doing, or ask questions!\n"
      ],
      "id": "ZTUcrmg79jcF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YC4A27Mh9jcO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "import os\n",
        "\n",
        "from tensorflow.keras.datasets import imdb\n",
        "print(keras.__version__)\n",
        "\n",
        "from tensorflow.keras import layers\n"
      ],
      "id": "YC4A27Mh9jcO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJVrATf09jcS"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "(train_data, train_labels), _ = imdb.load_data(num_words=10000)  #"
      ],
      "id": "sJVrATf09jcS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ET1BLbHs9jcT"
      },
      "outputs": [],
      "source": [
        "# Define function to vectorise dataset\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.\n",
        "    return results\n",
        "\n",
        "# Vectorise training data\n",
        "train_data = vectorize_sequences(train_data)\n",
        "print(len(train_data))"
      ],
      "id": "ET1BLbHs9jcT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the data is evenly split."
      ],
      "metadata": {
        "id": "-o06mh0hE7-4"
      },
      "id": "-o06mh0hE7-4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_b01QKn9jcU"
      },
      "outputs": [],
      "source": [
        "# Create original model architecture\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(16, activation=\"relu\"),\n",
        "    layers.Dense(16, activation=\"relu\"),\n",
        "    layers.Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# Train data\n",
        "history_original = model.fit(\n",
        "    train_data,\n",
        "    train_labels,\n",
        "    epochs=20,\n",
        "    batch_size=512,\n",
        "    validation_split=0.4\n",
        "    )"
      ],
      "id": "E_b01QKn9jcU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let’s try to replace it with this smaller model."
      ],
      "metadata": {
        "id": "gCSvigDqPzAj"
      },
      "id": "gCSvigDqPzAj"
    },
    {
      "cell_type": "code",
      "source": [
        "# Version of the model with lower capacity\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(4, activation=\"relu\"),\n",
        "    layers.Dense(4, activation=\"relu\"),\n",
        "    layers.Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "history_smaller_model = model.fit(\n",
        "    train_data, train_labels,\n",
        "    epochs=20,\n",
        "    batch_size=512,\n",
        "    validation_split=0.4)"
      ],
      "metadata": {
        "id": "zuWzuLsxP0-8"
      },
      "id": "zuWzuLsxP0-8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare validation losses of original model and smaller model\n",
        "\n",
        "val_loss_original = history_original.history[\"val_loss\"]\n",
        "val_loss_smaller_model = history_smaller_model.history[\"val_loss\"]\n",
        "epochs = range(1, 21)\n",
        "plt.plot(epochs, val_loss_original, \"b--\",\n",
        "label=\"Validation loss of original model\")\n",
        "plt.plot(epochs, val_loss_smaller_model, \"b-\",\n",
        "label=\"Validation loss of smaller model\")\n",
        "plt.title(\"Comparison of validation losses of the original and smaller models\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Validation loss\")\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "h3ZEOu_yP3Uo"
      },
      "id": "h3ZEOu_yP3Uo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the smaller model starts overfitting later than the reference model (after six epochs rather than four), and its performance degrades more slowly once it starts overfitting."
      ],
      "metadata": {
        "id": "VoaPKVG_T51e"
      },
      "id": "VoaPKVG_T51e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now add a model that has far more capacity than the problem warrants. While it is standard to work with models that are significantly overparameterized for what they’re trying to learn, there can definitely be such a thing as too much memorization capacity. You’ll know your model is too large if it\n",
        "starts overfitting right away and if its validation loss curve looks choppy with high variance (although choppy validation metrics could also be a symptom of using an unreliable validation process, such as a validation split that’s too small)."
      ],
      "metadata": {
        "id": "HjqTtevoUR3G"
      },
      "id": "HjqTtevoUR3G"
    },
    {
      "cell_type": "code",
      "source": [
        "# Version of the model with higher capacity\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(512, activation=\"relu\"),\n",
        "    layers.Dense(512, activation=\"relu\"),\n",
        "    layers.Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "history_larger_model = model.fit(\n",
        "    train_data,\n",
        "    train_labels,\n",
        "    epochs=20,\n",
        "    batch_size=512,\n",
        "    validation_split=0.4)"
      ],
      "metadata": {
        "id": "8NwqDMVrUxrw"
      },
      "id": "8NwqDMVrUxrw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare validation losses of original model and larger model\n",
        "\n",
        "val_loss_original = history_original.history[\"val_loss\"]\n",
        "val_loss_larger_model = history_larger_model.history[\"val_loss\"]\n",
        "epochs = range(1, 21)\n",
        "plt.plot(epochs, val_loss_original, \"b--\",\n",
        "label=\"Validation loss of original model\")\n",
        "plt.plot(epochs, val_loss_larger_model, \"b-\",\n",
        "label=\"Validation loss of larger model\")\n",
        "plt.title(\"Comparison of validation losses of the original and larger models\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "2ditrJmJP3Pj"
      },
      "id": "2ditrJmJP3Pj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bigger model starts overfitting almost immediately, after just one epoch, and it overfits much more severely. Its validation loss is also noisier. It gets training loss near zero very quickly. The more capacity the model has, the more quickly it can model the\n",
        "training data (resulting in a low training loss), but the more susceptible it is to overfitting (resulting in a large difference between the training and validation loss)."
      ],
      "metadata": {
        "id": "ZsKNSE41WOBK"
      },
      "id": "ZsKNSE41WOBK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weight Regularization\n",
        "\n",
        "Another way of avoiding overfitting is by using weight regularization. You may be familiar with the principle of Occam’s razor: given two explanations for something, the explanation most likely to be correct is the simplest one — the one that\n",
        "makes fewer assumptions. This idea also applies to the models learned by neural networks: given some training data and a network architecture, multiple sets of weight values (multiple models) could explain the data. Simpler models are less likely to overfit\n",
        "than complex ones.\n",
        "\n",
        "\n",
        "A simple model in this context is a model with fewer parameters.\n",
        "Thus, a common way to mitigate overfitting is to put constraints on the complexity of a model by forcing its weights to take only small values, which makes the distribution of weight values more regular. This is called *weight regularization*, and it’s\n",
        "done by adding to the loss function of the model a cost associated with having large weights. This cost comes in two flavors:\n",
        "\n",
        "1. **L1 regularization** — The cost added is proportional to the *absolute value of the weight coefficients* (the L1 norm of the weights).\n",
        "2. **L2 regularization** — The cost added is proportional to the square of the value of the weight coefficients *italicized text* (the L2 norm of the weights). L2 regularization is also called *weight decay* in the context of neural networks.\n",
        "\n",
        "In Keras, weight regularization is added by passing *weight regularizer instances* to layers as keyword arguments. Let’s add L2 weight regularization to our initial movie-review\n",
        "classification model."
      ],
      "metadata": {
        "id": "xPUZq7GPWot2"
      },
      "id": "xPUZq7GPWot2"
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding L2 weight regularization to the model\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(16, kernel_regularizer=regularizers.l2(0.002), activation=\"relu\"),\n",
        "    layers.Dense(16, kernel_regularizer=regularizers.l2(0.002), activation=\"relu\"),\n",
        "    layers.Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "history_l2_reg = model.fit(\n",
        "    train_data,\n",
        "    train_labels,\n",
        "    epochs=20,\n",
        "    batch_size=512,\n",
        "    validation_split=0.4)"
      ],
      "metadata": {
        "id": "0IM9YBTZP3J8"
      },
      "id": "0IM9YBTZP3J8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare validation losses of original model and larger model\n",
        "\n",
        "val_loss_original = history_original.history[\"val_loss\"]\n",
        "val_loss_l2_reg = history_l2_reg.history[\"val_loss\"]\n",
        "epochs = range(1, 21)\n",
        "plt.plot(epochs, val_loss_original, \"b--\",\n",
        "label=\"Validation loss of original model\")\n",
        "plt.plot(epochs, val_loss_l2_reg, \"b-\",\n",
        "label=\"Validation loss of L2-regularized model\")\n",
        "plt.title(\"Effect of L2 weight regularization on validation loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "20btVcqddsJ7"
      },
      "id": "20btVcqddsJ7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the model with `L2 regularization` has become much more resistant to overfitting than the reference model, even though both models have the same number of parameters."
      ],
      "metadata": {
        "id": "TAF8SJvmdWVg"
      },
      "id": "TAF8SJvmdWVg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "As an alternative to L2 regularization, you can use one of the following Keras weight regularizers."
      ],
      "metadata": {
        "id": "_wlCnJGge6Ah"
      },
      "id": "_wlCnJGge6Ah"
    },
    {
      "cell_type": "code",
      "source": [
        "# Different weight regularizers available in Keras\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "regularizers.l1(0.001)      #L1 regularization\n",
        "regularizers.l1_l2(l1=0.001, l2=0.001)    #Simultaneous L1 and L2 regularization"
      ],
      "metadata": {
        "id": "_IwPQ8IDe5R4"
      },
      "id": "_IwPQ8IDe5R4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code, `l2(0.002)` means every coefficient in the weight matrix of the layer will add `0.002 * weight_coefficient_value ** 2` to the total loss of the model. Note that because this penalty is only added at training time, the loss for this model will be much higher at training than at test time.\n",
        "\n",
        "**Note** that weight regularization is more typically used for *smaller deep learning models*. *Large deep learning models* tend to be so overparameterized that imposing constraints on weight values hasn’t much impact on model capacity and generalization. In\n",
        "these cases, a different regularization technique is preferred: *dropout*."
      ],
      "metadata": {
        "id": "0k6mfsRIcWo7"
      },
      "id": "0k6mfsRIcWo7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dropout\n",
        "\n",
        "*Dropout* is one of the most effective and most commonly used regularization techniques for neural networks. *Dropout*, applied to a layer, consists of randomly dropping out (setting to zero) a number of output features of the layer during training. Let’s say a\n",
        "given layer would normally return a vector `[0.2, 0.5, 1.3, 0.8, 1.1]` for a given input sample during training. After applying *dropout*, this vector will have a few zero entries distributed at random: for example, `[0, 0.5, 1.3, 0, 1.1]`.\n",
        "\n",
        "The *dropout* rate is the fraction of the features that are zeroed out; it’s usually set between `0.2 and 0.5`. At test time, no units are dropped out; instead, the layer’s output values are scaled down by a factor equal to the dropout rate, to balance for the fact that more units are active than at training time. The core idea is that introducing noise in the output values of a layer can break up happenstance patterns that aren’t significant."
      ],
      "metadata": {
        "id": "T0_HH1tVgf2X"
      },
      "id": "T0_HH1tVgf2X"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Keras, you can introduce *dropout* in a model via the Dropout layer, *which is applied to the output of the layer right before it*. Let’s add two Dropout layers in the IMDB model to see how well they do at reducing overfitting."
      ],
      "metadata": {
        "id": "YrCrKggpkTzF"
      },
      "id": "YrCrKggpkTzF"
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding dropout to the IMDB model\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(16, activation=\"relu\"),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(16, activation=\"relu\"),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "history_dropout = model.fit(\n",
        "    train_data,\n",
        "    train_labels,\n",
        "    epochs=20,\n",
        "    batch_size=512,\n",
        "    validation_split=0.4)"
      ],
      "metadata": {
        "id": "mHDSvD_agZEq"
      },
      "id": "mHDSvD_agZEq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare validation losses of original model and model with dropout\n",
        "\n",
        "val_loss_original = history_original.history[\"val_loss\"]\n",
        "val_loss_dropout = history_dropout.history[\"val_loss\"]\n",
        "epochs = range(1, 21)\n",
        "plt.plot(epochs, val_loss_original, \"b--\",\n",
        "label=\"Validation loss of original model\")\n",
        "plt.plot(epochs, val_loss_dropout, \"b-\",\n",
        "label=\"Validation loss of dropout-regularized model\")\n",
        "plt.title(\"Effect of dropout on validation loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "am175vmSgZxG"
      },
      "id": "am175vmSgZxG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, we notice a clear improvement over the original network.\n",
        "\n",
        "To recap, these are the most common ways to maximize generalization and prevent overfitting in neural networks:\n",
        "\n",
        "a) Get more training data, or better training data.\n",
        "\n",
        "b) Develop better features.\n",
        "\n",
        "c) Reduce the capacity of the model.\n",
        "\n",
        "d) Add weight regularization (for smaller models).\n",
        "\n",
        "e) Add dropout."
      ],
      "metadata": {
        "id": "_O2R1VMhmpKF"
      },
      "id": "_O2R1VMhmpKF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Brief Introduction to Convnets (aka Convolutional Neural Networks, or CNNs)\n",
        "\n",
        "As an introduction, let's take a quick look at a simple convnet example. It uses a convnet to classify MNIST digits, a task we performed in Chapter 2 using a densely connected network (our test accuracy then was around 97.8%). Convnets have been very\n",
        "successful at computer vision tasks.Even though this convnet we are about to buid will be basic, its accuracy will be way better than the densely connected network.\n",
        "\n",
        "A basic convnet is a stack of It’s a stack of `Conv2D` and `MaxPooling2D` layers. We will build the model using the *Functional API*, which was introduced in Chapter 7.\n",
        "\n",
        "Importantly, a convnet takes as input tensors of shape `(image_height, image_width, image_channels)` (not including the batch dimension). In this case, we'll configure the convnet to process inputs of size `(28, 28, 1)`, which is the format of MNIST images (image_channels=1 because the images are black and white; colour images have image_channels=3). We do this by passing the argument `input_shape = c(28, 28, 1)` to the first layer."
      ],
      "metadata": {
        "id": "ia3dgXHptgEE"
      },
      "id": "ia3dgXHptgEE"
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiating a small convnet\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "inputs = keras.Input(shape=(28, 28, 1))\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "jPskZtqPv2ls"
      },
      "id": "jPskZtqPv2ls",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MCCj6Es9jcq"
      },
      "outputs": [],
      "source": [
        "# Displaying model summary\n",
        "model.summary()"
      ],
      "id": "2MCCj6Es9jcq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vuob3uv69jcr"
      },
      "source": [
        "You can see that the output of every `Conv2D` and `MaxPooling2D` layer is a `rank-3` tensor of shape (height, width, channels). The width and height dimensions tend to shrink as you go deeper in the model. The number of channels is controlled by the first argument passed to the `Conv2D` layers `(32, 64, or 128)`. After the last `Conv2D` layer, we end up with an output of shape `(3, 3, 128)` — a `3 × 3` feature map of 128 channels.\n",
        "\n",
        "The next step is to feed this output into a densely connected\n",
        "classifier like those you’re already familiar with: a stack of Dense layers. These classifiers process vectors, which are 1D, whereas the current output is a rank-3 tensor. To bridge the gap, we flatten the `3D` outputs to `1D` with a Flatten layer before adding the Dense layers. Finally, we do `10-way` classification, so our last layer has 10 outputs and a softmax activation."
      ],
      "id": "Vuob3uv69jcr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now train the convnet on the MNIST digits, reusing a lot of the code from the `MNIST` example in Chapter 2. Because we’re doing `10-way` classification with a softmax output, we’ll use the `categorical crossentropy loss`, and because our labels are\n",
        "integers, we’ll use the sparse version, `sparse_categorical_crossentropy`."
      ],
      "metadata": {
        "id": "3b5jZUspydp-"
      },
      "id": "3b5jZUspydp-"
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the convnet on MNIST images\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images = train_images.reshape((60000, 28, 28, 1))\n",
        "train_images = train_images.astype(\"float32\") / 255\n",
        "test_images = test_images.reshape((10000, 28, 28, 1))\n",
        "test_images = test_images.astype(\"float32\") / 255\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "loss=\"sparse_categorical_crossentropy\",\n",
        "metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(train_images, train_labels, epochs=5, batch_size=64)"
      ],
      "metadata": {
        "id": "eKv9sX9nyc9W"
      },
      "id": "eKv9sX9nyc9W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next thing to do is to evaluate the model on the test data."
      ],
      "metadata": {
        "id": "QHIweuBszNLG"
      },
      "id": "QHIweuBszNLG"
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the convnet\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "metadata": {
        "id": "UnAVRnguzXLe"
      },
      "id": "UnAVRnguzXLe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IrpjNUG9jcx"
      },
      "source": [
        "This is much better than we achieved before!\n",
        "\n",
        "If you get to this point, look back at the summary of the convolutional model and try to explain the number of parameters at each layer. Don't forget there are still bias terms!\n",
        "\n",
        "Whereas the densely connected model from chapter 2 had a test accuracy of `97.8%`, the basic convnet has a test accuracy of 99.1%: we decreased the relative error rate by about 60%.\n"
      ],
      "id": "1IrpjNUG9jcx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uO1PJVNx9jcx"
      },
      "outputs": [],
      "source": [],
      "id": "uO1PJVNx9jcx"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}